# kaggle_models

---

This repository is your **central hub for machine learning models** developed for **Kaggle competitions and personal projects**. It's designed to store and organize a diverse collection of solutions, approaches, and predictive models across various problem types.

Whether you're looking for past work, learning new techniques, or simply sharing your creations, this repository aims to be a comprehensive resource showcasing practical machine learning applications.

---

### Common Machine Learning Models

Here's a concise list of frequently used machine learning models you'll often encounter in data science:

* **Supervised Learning (Works with labeled data):**
    * **For predicting continuous values (Regression):** Linear Regression, Ridge/Lasso Regression, Support Vector Regression (SVR), Decision Trees, Random Forests, Gradient Boosting Machines (like XGBoost, LightGBM, CatBoost), K-Nearest Neighbors (KNN).
    * **For predicting categories (Classification):** Logistic Regression, K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Decision Trees, Random Forests, Gradient Boosting Machines (XGBoost, LightGBM, CatBoost), Naive Bayes, Artificial Neural Networks (ANNs).

* **Unsupervised Learning (Finds patterns in unlabeled data):**
    * **For grouping similar data (Clustering):** K-Means, Hierarchical Clustering, DBSCAN, Gaussian Mixture Models (GMM).
    * **For reducing the number of features (Dimensionality Reduction):** Principal Component Analysis (PCA), t-SNE, UMAP.

* **Ensemble Methods (Combine multiple models for better performance):**
    * **Bagging:** Like **Random Forest**, where multiple models are trained independently.
    * **Boosting:** Such as **AdaBoost** and **Gradient Boosting** (including XGBoost), where models are built sequentially, correcting previous errors.
    * **Stacking:** Combines predictions from diverse models using a "meta-model."

---
