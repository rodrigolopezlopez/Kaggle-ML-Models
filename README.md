# Kaggle Machine Learning Models

---

This repository is **my central hub for machine learning models** developed for **Kaggle activities and personal projects**. I designed it to store and organize my diverse collection of solutions, approaches, and predictive models across various problem types.

Whether I'm looking back at past work, learning new techniques, or simply sharing my creations, this repository aims to be a comprehensive resource showcasing my practical machine learning applications.

---

### Common Machine Learning Models I Use

Here's a concise list of frequently used machine learning models you'll often find in my work:

* **Supervised Learning (Works with labeled data):**
    * **For predicting continuous values (Regression):** Linear Regression, Ridge/Lasso Regression, Support Vector Regression (SVR), Decision Trees, Random Forests, Gradient Boosting Machines (like XGBoost, LightGBM, CatBoost), K-Nearest Neighbors (KNN).
    * **For predicting categories (Classification):** Logistic Regression, K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Decision Trees, Random Forests, Gradient Boosting Machines (XGBoost, LightGBM, CatBoost), Naive Bayes, Artificial Neural Networks (ANNs).

* **Unsupervised Learning (Finds patterns in unlabeled data):**
    * **For grouping similar data (Clustering):** K-Means, Hierarchical Clustering, DBSCAN, Gaussian Mixture Models (GMM).
    * **For reducing the number of features (Dimensionality Reduction):** Principal Component Analysis (PCA), t-SNE, UMAP.

* **Ensemble Methods (Combine multiple models for better performance):**
    * **Bagging:** Like **Random Forest**, where multiple models are trained independently.
    * **Boosting:** Such as **AdaBoost** and **Gradient Boosting** (including XGBoost), where models are built sequentially, correcting previous errors.
    * **Stacking:** Combines predictions from diverse models using a "meta-model."

---
